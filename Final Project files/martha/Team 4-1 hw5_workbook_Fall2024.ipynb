{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b25213b-47db-495f-9d82-0271f06ad1a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## HW 5 - Page Rank\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2024`__\n",
    "\n",
    "> Updated: 10/29/2024\n",
    "\n",
    "This is also a Team Homework, using the same team as the Final Project. **What's the catch? There are 5 questions, each student in the team has to author at least one question.**\n",
    "\n",
    "**Clone this notebook to be able to run it!**\n",
    "\n",
    "\n",
    "In Weeks 9 and 10 you discussed key concepts related to graph based algorithms and implemented SSSP. In this final homework assignment you'll implement distributed PageRank using some data from Wikipedia. This homework it's a team homework, thus every team will only submit one notebook. \n",
    "\n",
    "By the end of this homework you should be able to:  \n",
    "* ... __compare/contrast__ adjacency matrices and lists as representations of graphs for parallel computation.\n",
    "* ... __explain__ the goal of the PageRank algorithm using the concept of an infinite Random Walk.\n",
    "* ... __define__ a Markov chain including the conditions underwhich it will converge.\n",
    "* ... __identify__ what modifications must be made to the web graph inorder to leverage Markov Chains.\n",
    "* ... __implement__ distributed PageRank in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e89eb06-bbe7-486f-a060-170c6c0fd81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ebf232-974c-41e9-82d7-a1e5cb2a73f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RUN CELL AS IS - Imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from graphframes import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dc6d576-9a10-45de-8255-b52263c902f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RUN CELL AS IS - Spark Context\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d98479ad-90ac-41f7-bc2c-e9d463eab53f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be685e8d-ff14-4c28-ae74-96b2fbbd38a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL AS IS. You should see all-pages-indexed-in.txt, all-pages-indexed-out.txt and indices.txt in the results. If you do not see these, please let an Instructor or TA know.\n",
    "DATA_PATH = \"dbfs:/mnt/mids-w261/HW5/\"\n",
    "display(dbutils.fs.ls(DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572c9067-af5d-49f1-b9da-b9b713680a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 1: Graph Processing Theory\n",
    "\n",
    "# Author: Martha\n",
    "\n",
    "## a. Distribute Graph Processing\n",
    "Chapter 5 from Lin & Dyer gave you a high level introduction to graph algorithms and concernts that come up when trying to perform distributed computations over them. The questions below are designed to make sure you captured the key points from this reading. \n",
    "\n",
    "### Q1.a Tasks:\n",
    "* __a) Multiple Choice:__ Other than their size/scale, what makes graphs uniquely challenging to work with in the map-reduce paradigm? *(__HINT__: Don't think in terms of any specific algorithm. Think in terms of the nature of the graph data structure itself).*\n",
    "\n",
    "## b. Representing Graphs\n",
    "In class you saw examples of adjacency matrix and adjacency list representations of graphs. These data structures were probably familiar from HW3, though we hadn't before talked about them in the context of graphs. In this question we'll discuss some of the tradeoffs associated with these representations. __`NOTE:`__ We'll use the graph from Figure 5.1 in Lin & Dyer as a toy example. For convenience in the code below we'll label the nodes `A`, `B`, `C`, `D`, and `E` instead of \\\\(n{_1}\\\\), \\\\(n{_2}\\\\), etc but otherwise you should be able to follow along & check our answers against those in the text.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/HW5/Lin-Dyer-graph-Q1.png?raw=true\" width=50%>\n",
    "\n",
    "### Q1.b Tasks:\n",
    "\n",
    "* __a) Fill in the blanks:__ Relatively speaking, is the graph you described in Figure 5.1 in Lin & Dyer \"sparse\" or \"dense\"?  Explain how sparsity/density impacts the adjacency matrix and adjacency list representations of a graph. \n",
    "* __b) Fill in the blanks:__ Run the provided code to create and plot our toy graph. Is this graph directed or undirected? Explain how the adjacency matrices for directed graphs will differ from those of undirected graphs.\n",
    "* __c) Code:__ Fill in the missing code to complete the function `get_adj_matr()`.\n",
    "* __d) Code:__ Fill in the missing code to complete the function `get_adj_list()`.\n",
    "* __e) Multiple Choice:__ Which is the correct edge list for node `B`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4354a257-a47f-4f28-a294-b978e90b557f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q1.a Student Answers:\n",
    "> __a)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "* **It can be hard to represent graphs as distinct records that can be processed separately from each other.**\n",
    "* Graphs contain nodes and edges, which cannot easily be converted into key-value pairs.\n",
    "* You need to store the entire graph in memory, so we can only work with small datasets.\n",
    "\n",
    "### Q1.b Student Answers\n",
    "> __a)__ Fill in the blanks with any of these possible statements based on the following category.\n",
    "  > * type = [**sparse**, dense]. Sparse graph has O(n) edges, where n is the number of vertices. In this case we have 9 edges and 5 nodes (25 possible edges), therefore 9/25 is sparse. \n",
    "  > * number = [N^2 edges or 5^2 = 25, N(N-1) edges or 5*4 = 20]\n",
    "  > * efficiency = [**more efficient**, less efficient]. \n",
    "\n",
    "* This is a relatively sparse matrix. If self loops are allowed, then we could have a total of N^2 edges or 5^2 = 25 edges. Otherwise, if self loops are not allowed, we could have N(N-1) edges or 5*4 = 20 edges. Out of a total possible edges, this graph only has 9 edges. For sparse graphs, their adjacency list representation will be more efficient than their adjacency matrix (memory wise). \n",
    "\n",
    "\n",
    "> __b)__ Fill in the blanks with any of these possible statements based on the following category.\n",
    "  > * graph_type = [directed, undirected]\n",
    "  > * symmetric = [asymmetric, symmetric]\n",
    "\n",
    "* This is a **directed** graph. Because we see the edge ('A', 'B') in the graph, but there is no corresponding ('B', 'A'). For directed graphs, the adjacency matrix will be **asymmetric**. Whereas for undirected graphs, it will be **symmetric**. \n",
    "\n",
    "> __e)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "* **['C','E']**\n",
    "* ['D']\n",
    "* ['A', 'B', 'C']\n",
    "* ['B', 'D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46856c77-76f7-48c5-b8ba-4e9b77c701ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 1b.a - a graph is just a list of nodes and edges (RUN THIS CELL AS IS)\n",
    "TOY_GRAPH = {'nodes':['A', 'B', 'C', 'D', 'E'],\n",
    "             'edges':[('A', 'B'), ('A', 'D'), ('B', 'C'), ('B', 'E'), ('C', 'D'), \n",
    "                      ('D', 'E'), ('E', 'A'),('E', 'B'), ('E', 'C')]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b417fa64-a875-472e-9ec1-421cc3a42311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 1b.a - simple visualization of our toy graph using nx (RUN THIS CELL AS IS)\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(TOY_GRAPH['nodes'])\n",
    "G.add_edges_from(TOY_GRAPH['edges'])\n",
    "display(nx.draw_networkx(G, pos=nx.circular_layout(G), with_labels=True, alpha = 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7318d4-2935-488b-bdeb-10a70e8971fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 1b.c - adjacency matrix function\n",
    "def get_adj_matr(graph):\n",
    "    \"\"\"\n",
    "    Function to create an adjacency matrix representation of a graph.\n",
    "    arg:\n",
    "        graph - (dict) of 'nodes' : [], 'edges' : []\n",
    "    returns:\n",
    "        pd.DataFrame with entry i,j representing an edge from node i to node j\n",
    "    \"\"\"\n",
    "    n = len(graph['nodes'])\n",
    "    adj_matr = pd.DataFrame(0, columns = graph['nodes'], index = graph['nodes'])\n",
    "    ############### YOUR CODE HERE ##################\n",
    "    for edge in graph['edges']:\n",
    "        adj_matr.at[edge[0], edge[1]] = 1\n",
    "    ############### (END) YOUR CODE #################\n",
    "    return adj_matr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a52b176-0d72-4f67-b4e5-3a390cd7573f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 1b.c - take a look (RUN THIS CELL AS IS)\n",
    "TOY_ADJ_MATR = get_adj_matr(TOY_GRAPH)\n",
    "print(TOY_ADJ_MATR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05099c54-3a4c-4c1e-92eb-382fdd713851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 1b.d - adjacency list function\n",
    "def get_adj_list(graph):\n",
    "    \"\"\"\n",
    "    Function to create an adjacency list representation of a graph.\n",
    "    arg:\n",
    "        graph - (dict) of 'nodes' : [], 'edges' : []\n",
    "    returns:\n",
    "        dictionary of the form {node : [list of edges]}\n",
    "    \"\"\"\n",
    "    adj_list = {node: [] for node in graph['nodes']}\n",
    "    ############### YOUR CODE HERE ##################\n",
    "    for edge in graph['edges']:\n",
    "        adj_list[edge[0]].append(edge[1])\n",
    "    ############### (END) YOUR CODE #################\n",
    "    return adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cac0efa-176d-4bc4-811b-24d6a2f242fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 1b.d - take a look (RUN THIS CELL AS IS)\n",
    "TOY_ADJ_LIST = get_adj_list(TOY_GRAPH)\n",
    "print(TOY_ADJ_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab1715e-4323-4946-b950-a05b879d1c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 2: Markov Chains, Random Walks and PageRank\n",
    "# Author: James\n",
    "\n",
    "## a. Markov Chains and Random Walks\n",
    "As you know from your readings and in class discussions, the PageRank algorithm takes advantage of the machinery of Markov Chains to compute the relative importance of a webpage using the hyperlink structure of the web (we'll refer to this as the 'web-graph'). A Markov Chain is a discrete-time stochastic process. The stochastic matrix has a principal left eigen vector corresponding to its largest eigen value which is one. A Markov chain's probability distribution over its states may be viewed as a probability vector. This steady state probability for a state is the PageRank of the corresponding webpage. In this question we'll briefly discuss a few concepts that are key to understanding the math behind PageRank. \n",
    "\n",
    "General note: We use a sped-up convergence method for the `power_iteration()` function. You should speed up convergence by multiplying by the tMatrix twice. `tMatrix = tMatrix.dot(tMatrix)`. So instead of:\n",
    "\n",
    "```\n",
    "Step 0: xInit * tMatrix\n",
    "Step 1: xInit * tMatrix^2\n",
    "Step 2: xInit * tMatrix^3\n",
    "Step 3: xInit * tMatrix^4\n",
    "...\n",
    "```\n",
    "\n",
    "you should instead consider\n",
    "\n",
    "```\n",
    "Step 0: xInit * tMatrix\n",
    "Step 1: xInit * tMatrix^2\n",
    "Step 2: xInit * tMatrix^4\n",
    "Step 3: xInit * tMatrix^8\n",
    "...\n",
    "```\n",
    "\n",
    "This may vary slightly from what you see in Demo 10\n",
    "\n",
    "### Q2.a Tasks:\n",
    "\n",
    "* __a) Multiple Choice:__ It is common to explain PageRank using the analogy of a web surfer who visits a page, randomly clicks a link on that page, and repeats ad infinitum. In the context of this hypothetical infinite random walk across web pages on the internet, which of the following choices most clearly describes the event that the __teleportation__ represents?\n",
    "\n",
    "* __b) Fill in the blanks:__ What is the \"Markov Property\" and what does it mean in the context of PageRank?\n",
    "\n",
    "* __c) Fill in the blanks:__ A Markov chain consists of \\\\(n\\\\) states plus an \\\\(n\\times n\\\\) transition probability matrix. In the context of PageRank & a random walk over the WebGraph, what are the \\\\(n\\\\) states? what implications does this have about the size of the transition matrix?\n",
    "\n",
    "* __d) Code + Numerical Answer:__ Fill in the code below to compute the transition matrix for the toy graph from question 1. What is the value in the middle column of the last row (the probability of transitioning from node `E` to `C`)? Include the leading digit in front of the decimal, and round the number to at least 4 decimal places. Examples: 1.0000 or 0.1234. [__`HINT:`__ _It should be right stochastic. Using numpy this calculation can be done in one line of code._]\n",
    "\n",
    "* __e) Code:__ To compute the stable state distribution (i.e. PageRank) of a \"nice\" graph we can apply the power iteration method - repeatedly multiplying the transition matrix by itself, until the values no longer change. Apply this strategy to your transition matrix from `part d` to find the PageRank for each of the pages in your toy graph. Your code should print the results of each iteration. \n",
    "    * __`NOTE 1:`__ _this is a naive approach, we'll unpack what it means to be \"nice\" in the next question_.\n",
    "    * __`NOTE 2:`__ _no need to implement a stopping criteria, visual inspection should suffice_.\n",
    "    * __`NOTE 3:`__ _refer to the General note section above, and use the `tMatrix = tMatrix.dot(tMatrix)` approach for faster convergence_.\n",
    "\n",
    "* __f) Numerical Answer:__ How many iterations does it take to converge? Define convergence as the step where you can identify conevergence. Example: If step 4 the value is 0.435 and in step 5 the value is also 0.435, you identified convergence in step 5.\n",
    "\n",
    "* __g) Multiple Choice:__ Which node is the least 'central' (i.e., it has the lowest ranked)?\n",
    "\n",
    "## b. Page Rank Theory\n",
    "Seems easy right? Unfortunately applying this power iteration method directly to the web-graph actually runs into a few problems. In this question we'll tease apart what we meant by a 'nice graph' from before and highlight key modifications we'll have to make to the web-graph when performing PageRank. To start, we'll look at what goes wrong when we try to repeat our strategy from before on a 'not nice' graph.\n",
    "\n",
    "__`Additional References:`__ http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html\n",
    "\n",
    "### Q2.b Tasks:\n",
    "\n",
    "* __a) Code + Multiple Choice:__ Run the provided code to create and plot our 'not nice' graph. Fill in the missing code to compute its transition matrix & run the power iteration method from before. What are the first values in steps 1, 2, and 3 of the power iteration method? [__`HINT:`__ _We start the iteration at step number 0. If you start your iteration at step number 1, then you should answer with the values from step 2, 3, and 4 instead_]\n",
    "\n",
    "* __b) Multiple Choice:__ What is wrong with what you see in part a? [__`HINT:`__ _there is a visible underlying reason that it isn't converging... try adding up the probabilities in the state vector after each iteration._]\n",
    "\n",
    "* __c) Multiple Choice:__  Identify the dangling node in this 'not nice' graph and explain how this node causes the problem in 'a'. How could we modify the transition matrix after each iteration to prevent this problem?\n",
    "\n",
    "* __d) Multiple Choice:__ What does it mean for a graph to be irreducible? Is the webgraph naturally irreducible? Choose your reasoning.\n",
    "\n",
    "* __e) Multiple Choice:__ What does it mean for a graph to be aperiodic? Is the webgraph naturally aperiodic? Choose your reasoning.\n",
    "\n",
    "* __f) Multiple Choices:__ What modifications to the webgraph does PageRank make in order to guarantee aperiodicity and irreducibility? [__`HINT:`__ _select 2 answers_]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1ca6f4-e148-4b18-8a38-4205af708860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q2.a Student Answers:\n",
    "> __a)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "* Randomly traveling to a linked site from the current page.\n",
    "* Deterministically going to a linked site from a the current page.\n",
    "* **Randomly traveling to a non-linked site from the current page.**\n",
    "* A self-loop in which the user clicks a link to return the same page.\n",
    "\n",
    "> __b)__ Fill in the blanks with any of these possible statements based on the following category.\n",
    "  > * property = [memorylessness, randomness]\n",
    "  > * assumption = [the probability of transitioning from one page to another is stable regardless of past browsing history, the probability of transitioning from one page to another changes according to the past browsing history]\n",
    "\n",
    "* The Markov Property is the property of **memorylessness**. In the context of Page Rank, this is the assumption that **the probability of transitioning from one page to another is stable regardless of past browsing history**.\n",
    "\n",
    "> __c)__ Fill in the blanks with any of these possible statements based on the following category.\n",
    "  > * states = [links, webpages]\n",
    "  > * size = [large, relatively small]\n",
    "\n",
    "* The n states are the **webpages** Therefore, the size of the transition matrix will be **large**\n",
    "\n",
    "> __d)__ Numeric Answer: **0.3333**\n",
    "\n",
    "> __f)__ Numeric Answer: **6**\n",
    "\n",
    "> __g)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "* **A**\n",
    "* B\n",
    "* C\n",
    "* D\n",
    "* E\n",
    "\n",
    "### Q2.b Student Answers:\n",
    "> __a)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "* **[0, 0, 0]**\n",
    "* [0.16666667, 0.02777778, 0.0007716]\n",
    "* [0.16666667, 0.16666667, 0.16666667]\n",
    "* [0.16666667, 0, 0.16666667]\n",
    "\n",
    "> __b)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "* Each column should represent a probability distribution but these don't sum to 1. In fact the more iterations we run, the higher their sum is.\n",
    "* **Each column should represent a probability distribution but these don't sum to 1. In fact the more iterations we run, the lower their sum is.**\n",
    "\n",
    "\n",
    "> __c)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "* A dangling node is a node with outlinks but no inlinks. We need to redistribute the dangling mass to maintain stochasticity.\n",
    "* A dangling node is a node with no outlinks or inlinks. We need to redistribute the dangling mass to maintain stochasticity.\n",
    "* **A dangling node is a node with inlinks but no outlinks. We need to redistribute the dangling mass to maintain stochasticity.**\n",
    "\n",
    "> __d)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "* **Irreducibility: There must be a sequence of transitions of non-zero probability from any state to any other. In other words, the graph has to be connected (a path exists from all vertices to all vertices). No, the webgraph is not irreducible as it will have disconnected segments.**\n",
    "* Irreducibility: There must be a sequence of transitions of non-zero probability from any state to any other. In other words, the graph has to be connected (a path exists from all vertices to all vertices). Yes, the webgraph is irreducible as all segments are connected.\n",
    "\n",
    "> __e)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "* Aperiodicity: States are not partitioned into sets such that all state transitions occur cyclicly from one set to another. No, the web graph is not aperiodic as all links are outlinks.\n",
    "* **Aperiodicity: States are not partitioned into sets such that all state transitions occur cyclicly from one set to another. Yes, the web graph is aperiodic as all we need is a single page with a link back to itself. An anchor link is one such link.**\n",
    "\n",
    "> __f)__ Highlight the correct answers - 2 options (Add ** before and after your selected choice):\n",
    "* **teleportation**\n",
    "* oversampling\n",
    "* **dampening**\n",
    "* deterministic random surfer behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "395d0663-da14-44f6-a335-e2a13611249c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 2.a.d - recall what the adjacency matrix looked like (RUN THIS CELL AS IS)\n",
    "TOY_ADJ_MATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c271a169-6581-479c-8733-80d644bfd9d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 2.a.d - use TOY_ADJ_MATR to create a right stochastic transition matrix for this graph\n",
    "################ YOUR CODE HERE #################\n",
    "# To do with numpy in one line, we could also do it like this I think\n",
    "# Compute row sums (out-degrees)\n",
    "row_sums = TOY_ADJ_MATR.to_numpy().sum(axis=1, keepdims=True)\n",
    "\n",
    "# Handle dangling nodes (rows with sum 0)\n",
    "row_sums[row_sums == 0] = 1  # Prevent division by zero by setting sum to 1\n",
    "\n",
    "# Normalize each row to create the transition matrix\n",
    "transition_matrix = TOY_ADJ_MATR / row_sums\n",
    "\n",
    "################ (END) YOUR CODE #################\n",
    "print(transition_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf8b949-fc49-42ba-8e42-bdd46faf4a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To do with numpy in one line, we could also do it like this I think\n",
    "row_sums = TOY_ADJ_MATR.sum(axis=1)\n",
    "transition_matrix = (TOY_ADJ_MATR.T / row_sums).T  # Divide each row by its sum\n",
    "print(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff2c165-6a3b-413b-9497-3d0dd6c1c98f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 2.a.e - compute the steady state using the transition matrix \n",
    "def power_iteration(xInit, tMatrix, nIter, verbose = True):\n",
    "    \"\"\"\n",
    "    Function to perform the specified number of power iteration steps to \n",
    "    compute the steady state probability distribution for the given\n",
    "    transition matrix.\n",
    "    \n",
    "    Args:\n",
    "        xInit     - (n x 1 array) representing inial state\n",
    "        tMatrix  - (n x n array) transition probabilities\n",
    "        nIter     - (int) number of iterations\n",
    "    Returns:\n",
    "        state_vector - (n x 1 array) representing probability \n",
    "                        distribution over states after nSteps.\n",
    "    \n",
    "    NOTE: if the 'verbose' flag is on, your function should print the step\n",
    "    number and the current matrix at each iteration.\n",
    "    \"\"\"\n",
    "    state_vector = xInit\n",
    "    ################ YOUR CODE HERE #################\n",
    "    for ix in range(nIter):\n",
    "        new_state_vector = state_vector.dot(tMatrix)\n",
    "        state_vector = new_state_vector\n",
    "\n",
    "        # speed up convergence by multiplying tMatrix by itself\n",
    "        tMatrix = tMatrix.dot(tMatrix)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Step {ix}: {state_vector}; sum of vector: {np.sum(state_vector)}')    \n",
    "    \n",
    "    ################ (END) YOUR CODE #################\n",
    "    return state_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2000ad16-0adb-4607-b1fa-7529993f1f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 2.a.e - run 10 steps of the power_iteration (RUN THIS CELL AS IS)\n",
    "xInit = np.array([1.0, 0, 0, 0, 0]) # note that this initial state will not affect the convergence states\n",
    "states = power_iteration(xInit, transition_matrix, 10, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4391eaa-5068-4a23-ac08-121ca1add4e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "__`Expected Output for part 2.a.e after 10 iterations:`__  \n",
    ">Steady State Probabilities:\n",
    "```\n",
    "[0.10526316 0.15789474 0.18421053 0.23684211 0.31578947] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b955938-299e-4145-be86-695d55b45e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 2.b.a - run this code to create a second toy graph (RUN THIS CELL AS IS)\n",
    "TOY2_GRAPH = {'nodes':['A', 'B', 'C', 'D', 'E'],\n",
    "              'edges':[('A', 'B'), ('A', 'C'), ('A', 'D'), ('B', 'D'), \n",
    "                       ('B', 'E'), ('C', 'A'), ('C', 'E'), ('D', 'B')]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c954c14c-faa3-4791-bf58-316faf086586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 2.b.a - simple visualization of our test graph using nx (RUN THIS CELL AS IS)\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(TOY2_GRAPH['nodes'])\n",
    "G.add_edges_from(TOY2_GRAPH['edges'])\n",
    "display(nx.draw_networkx(G, pos=nx.circular_layout(G), with_labels=True, alpha = 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "214fe212-37a7-42ad-9123-8d18ac7916cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 2.b.a - run 10 steps of the power iteration method here\n",
    "# HINT: feel free to use the functions get_adj_matr() and power_iteration() you wrote above\n",
    "################ YOUR CODE HERE #################\n",
    "adj_matr = get_adj_matr(TOY2_GRAPH) # replace with your code\n",
    "print(adj_matr)\n",
    "\n",
    "# Compute row sums (out-degrees)\n",
    "row_sums = adj_matr.to_numpy().sum(axis=1, keepdims=True)\n",
    "\n",
    "# Handle dangling nodes (rows with sum 0)\n",
    "row_sums[row_sums == 0] = 1  # Prevent division by zero by setting sum to 1\n",
    "\n",
    "# Normalize each row to create the transition matrix\n",
    "trans_matr = adj_matr / row_sums\n",
    "\n",
    "print(trans_matr)\n",
    "\n",
    "print('\\nStarting power iteration:')\n",
    "state = power_iteration(xInit, trans_matr, 10, verbose = True) # replace with your code\n",
    "################ (END) YOUR CODE #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5a50474-f7a3-4ab0-b2cd-d34c19b56d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 3: Data and EDA\n",
    "# Author: Rini\n",
    "\n",
    "## About the Data\n",
    "The main dataset for this data consists of a subset of a 500GB dataset released by AWS in 2009. The data includes the source and metadata for all of the Wikimedia wikis. You can read more here: \n",
    "> https://aws.amazon.com/blogs/aws/new-public-data-set-wikipedia-xml-data. \n",
    "\n",
    "Use the cells below to download the wikipedia data and a test file for use in developing your PageRank implementation (note that we'll use the 'indexed out' version of the graph) and to take a look at the files.\n",
    "\n",
    "## a. EDA - Number of nodes:\n",
    "As usual, before we dive in to the main analysis, we'll peform some exploratory data anlysis to understand our dataset. Please use the test graph that you downloaded to test all your code before running the full dataset.\n",
    "\n",
    "### Q3a. Tasks:\n",
    "* __a) Fill in the blanks:__ What is the format of the raw data? What does the first value represent? What does the second part of each line represent? [__`HINT:`__ _no need to go digging here, just visually inspect the outputs of the head commands that we ran after loading the data above._]\n",
    "* __b) Multiple Choice:__ Run the provided bash command to count the number of records in the raw dataset. Explain why this is _not_ the same as the number of total nodes in the graph.\n",
    "* __c) Code:__ In the space provided below write a Spark job to count the _total number_ of nodes in this graph. \n",
    "* __d) Numerical Answer:__ How many dangling nodes are there in this wikipedia graph? [__`HINT:`__ _you should not need any code to answer this question._]\n",
    "\n",
    "## b. EDA - Out-Degree distribution:\n",
    "As you've seen in previous homeworks the computational complexity of an implementation depends not only on the number of records in the original dataset but also on the number of records we create and shuffle in our intermediate representation of the data. The number of intermediate records required to update PageRank is related to the number of edges in the graph. In this question you'll compute the average number of hyperlinks on each page in this data and visualize a distribution for these counts (the out-degree of the nodes). \n",
    "\n",
    "### Q3b. Tasks:\n",
    "* __a) Code:__ In the space provided below write a Spark job to stream over the data and compute all of the following information:\n",
    "  * Count the number of out-degree for each __non-dangling node__ and return the names of the top 10 pages with the most hyperlinks\n",
    "  * Find the average out-degree for all __non-dangling nodes__ in the graph\n",
    "  * Take a 1000 point sample of these out-degree counts and plot a histogram of the result. \n",
    "* __b) Numerical Answer:__ What is the average out-degree of the `testRDD`?\n",
    "* __c) Multiple Choice:__ What is the top node by out-degree of the `wikiRDD`?\n",
    "* __d) Fill in the blanks:__ In the context of the PageRank algorithm, how is information about a node's out degree used?\n",
    "* __e) Multiple Choices:__ What does it mean if a node's out-degree is 0? In PageRank how will we handle these nodes differently than others? Select all that apply. [__Hint:__ _select 3 answers_]\n",
    "\n",
    "__`NOTE:`__ Please observe scalability best practices in the design of your code & comment your work clearly. You will be graded on both the clarity and the design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa941a8-9b04-4530-b7af-4a068a2884ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# open test_graph.txt file to see format (RUN THIS CELL AS IS)\n",
    "with open('/dbfs/mnt/mids-w261/HW5/test_graph.txt', \"r\") as f_read:\n",
    "  for line in f_read:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221607ca-4cc1-4559-af28-6fd16297ec28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the data into Spark RDDs for convenience of use later (RUN THIS CELL AS IS)\n",
    "DATA_PATH = 'dbfs:/mnt/mids-w261/HW5/'\n",
    "testRDD = sc.textFile(DATA_PATH +'test_graph.txt')\n",
    "indexRDD = sc.textFile(DATA_PATH + '/indices.txt')\n",
    "wikiRDD = sc.textFile(DATA_PATH + '/all-pages-indexed-out.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab53dc55-a862-4c17-8ffb-7b56b9b2a3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display testRDD (RUN THIS CELL AS IS)\n",
    "testRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdeec119-bac4-430b-b862-bbf8b8068f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display indexRDD (RUN THIS CELL AS IS)\n",
    "indexRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0833912-6f83-4191-9550-39c2d9b8fbf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display wikiRDD (RUN THIS CELL AS IS)\n",
    "wikiRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8a372b3-1d7e-4538-b546-b24c53d97772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q3.a Student Answers:\n",
    "> __a)__ Fill in the blanks with any of these possible statements based on the following category.\n",
    "  > * type = [a dictionary, an adjacency list]\n",
    "  > * value = [node id, node count]\n",
    "  > * part = [a tuple of out-edges, a dictionary of linked pages (neighbor nodes) and the number of times that page is linked]\n",
    "* The raw data is in the format of **an adjacency list**.\n",
    "* The first value of each line represents **node id**.\n",
    "* The second part of each line represents **dictionary of linked pages (neighbor nodes) and the number of times that page is linked**.\n",
    "\n",
    "> __b)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "* Webpages (i.e. nodes) that are not referenced by a hyperlink (i.e. in-edges) won't have a record in this raw representation of the graph.\n",
    "* **Webpages (i.e. nodes) that don't have any hyperlinks (i.e. out-edges) won't have a record in this raw representation of the graph.**\n",
    "\n",
    "> __d)__ Numeric Answer: 9410987\n",
    "\n",
    "### Q3.b Student Answers:\n",
    "> __b)__ Numeric Answer: 2.2\n",
    "\n",
    "> __c)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "\n",
    "* '7858931'\n",
    "* **'7804599'**\n",
    "* '7705822'\n",
    "* '11185362'\n",
    "* '5760310'\n",
    "\n",
    "> __d)__ Fill in the blanks with any of these possible statements based on the following category.\n",
    "  > * value = [probability of random jump, total inbound links, current rank (or current mass)]\n",
    "  > * node = [each of its neighbor(s), every node in the network]\n",
    "\n",
    "  * The outdegree is used in calculating each node's \"contribution\" to its neighbors upon each iteration/update step. Specifically we take a node's **current rank (or current mass)**, divide it by the out-degree, and then redistribute that value to **each of its neighbor(s)** to get added up.\n",
    "\n",
    "> __e)__ Highlight the correct answers (Add ** before and after your selected choice):\n",
    "\n",
    "* **Nodes with out-degree 0 are dangling nodes (also known as 'sinks').**\n",
    "* A node with out-degree 0 has a full adjacency list which contains all other nodes in the network.\n",
    "* **Without modification of distribution, the mass from the nodes with out-degree 0 is not distributed to other nodes. The total rank (or accumulated mass) from all nodes will be less than 1, and using Markov chain will not converge.**\n",
    "* Without modification of distribution, the mass from the nodes with out-degree 0 is randomly distributed to other nodes using teleport vector.\n",
    "* **In PageRank, we redistribute the mass from nodes with out-degree 0 evenly across the rest of the graph in PageRank.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03cc8a6-c2f9-4ef5-bf04-6f391f8fc130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 3.a.b - count the number of records in the raw data (RUN THIS CELL AS IS)\n",
    "# 5781290 - Approx. time: 20.55 seconds\n",
    "print(wikiRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a52d86-b9bd-440b-a5c5-1fe7cca822d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 3.a.c - write your Spark job here (compute total number of nodes)\n",
    "def count_nodes(dataRDD):\n",
    "    \"\"\"\n",
    "    Spark job to count the total number of nodes.\n",
    "    Returns: integer count \n",
    "    \"\"\"    \n",
    "    ############## YOUR CODE HERE ###############\n",
    "    def extract_nodes(line):\n",
    "        source = line.split('\\t')[0]\n",
    "        dest_dict = eval(line.split('\\t')[1])\n",
    "        return [source] + list(dest_dict.keys())\n",
    "\n",
    "    totalCount = dataRDD.flatMap(extract_nodes).distinct().count()    \n",
    "    ############## (END) YOUR CODE ###############   \n",
    "    return totalCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3778d2a6-13c3-4c84-8eaf-c9a8a3f6a112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 3.a.c - run your counting job on the test file (RUN THIS CELL AS IS)\n",
    "# Approx time: 0.41 seconds\n",
    "tot = count_nodes(testRDD)\n",
    "print(f'Total Nodes: {tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cad9491-9628-4629-ac57-9f035b8971c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 3.a.c - run your counting job on the full file (RUN THIS CELL AS IS)\n",
    "# Approx time: 6.35 minutes\n",
    "tot = count_nodes(wikiRDD)\n",
    "print(f'Total Nodes: {tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792f4f88-ace0-4344-9150-5809a8463750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 3.a.d - number of dangling nodes - Use your previous results! You can hard code the number using arithmetic of previous results\n",
    "############## YOUR CODE HERE ###############\n",
    "num_dangling = 9410987\n",
    "############## (END) YOUR CODE ###############   \n",
    "\n",
    "# RUN AS IS\n",
    "fig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "data = [5781290, num_dangling]\n",
    "labels = [\"regular\", \"dangling\"]\n",
    "\n",
    "def func(pct, allvals):\n",
    "    absolute = int(pct/100.*np.sum(allvals))\n",
    "    return \"{:.1f}%\\n{:d}\".format(pct, absolute)\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data),\n",
    "                                  textprops=dict(color=\"w\"))\n",
    "ax.legend(wedges, labels,\n",
    "          title=\"Node Type\",\n",
    "          loc=\"center left\",\n",
    "          bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "plt.setp(autotexts, size=12, weight=\"bold\")\n",
    "ax.set_title(\"Ratio of dangling nodes\")\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3733e4c5-5f90-4c11-b3a1-7f3bb0e5872a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 3.b.a - write your Spark job here (compute average in-degree, etc)\n",
    "def count_degree(dataRDD, n):\n",
    "    \"\"\"\n",
    "    Function to analyze out-degree of nodes in a graph.\n",
    "    Returns: \n",
    "        top  - (list of 10 tuples) nodes with most edges\n",
    "        avgDegree - (float) average out-degree for non-dangling nodes\n",
    "        sampledCounts - (list of integers) out-degree for n randomly sampled non-dangling nodes\n",
    "    \"\"\"\n",
    "    # helper func to parse each line of the RDD\n",
    "    def parse(line):\n",
    "        node, edges = line.split('\\t')\n",
    "        return (node, ast.literal_eval(edges))\n",
    "    \n",
    "    ############## YOUR CODE HERE ###############\n",
    "    nodes = dataRDD.map(parse).cache()  # cache since we will continue to use this downstream\n",
    "    \n",
    "    # Count the number of out-degree nodes for each node\n",
    "    degreeCounts = nodes.map(lambda x: (x[0], sum(x[1].values())))\n",
    "\n",
    "    # Get the top 10 nodes with the highest out-degree\n",
    "    top = degreeCounts.takeOrdered(10, key=lambda x: -x[1]) \n",
    "    \n",
    "    # Calculate the total sum and count of out-degrees\n",
    "    degreeStats = (degreeCounts\n",
    "                .map(lambda x: x[1])  # just the counts\n",
    "                .aggregate((0, 0),  # (sum, count)\n",
    "                        lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
    "                        lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])))\n",
    "   \n",
    "    # Compute the average out-degree\n",
    "    avgDegree = degreeStats[0] / float(degreeStats[1])\n",
    "\n",
    "    # Sample n out-degree counts without replacement\n",
    "    sampledCounts = (degreeCounts\n",
    "                .map(lambda x: x[1])  # just the counts \n",
    "                .takeSample(False, n))  # False = sample without replacement\n",
    "    ############## (END) YOUR CODE ###############\n",
    "    \n",
    "    return top, avgDegree, sampledCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d65eb6c1-f143-4322-a4a6-5aaf7e14595c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "testRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e280f486-8536-44c1-9d73-e0ab09f62046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 3.b.b - run your job on the test file (RUN THIS CELL AS IS)\n",
    "# Approx time: 1.20 seconds\n",
    "test_results = count_degree(testRDD,10)\n",
    "print(\"Average out-degree: \", test_results[1])\n",
    "print(\"Top 10 nodes (by out-degree:)\\n\", test_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "073bb923-bf6c-4c7b-9a04-c9ccf33fd58b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 3.b.b - plot results from test file (RUN THIS CELL AS IS)\n",
    "plt.hist(test_results[2], bins=10)\n",
    "plt.title(\"Distribution of Out-Degree\")\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "911b66e0-01ce-483e-b471-292f3b86001a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 3.b.c - run your job on the full file (RUN THIS CELL AS IS)\n",
    "# Approx time: 3.69 minutes\n",
    "full_results = count_degree(wikiRDD,1000)\n",
    "print(\"Average out-degree: \", full_results[1])\n",
    "print(\"Top 10 nodes (by out-degree:)\\n\", full_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace6cf84-9e29-4cc7-9ce7-f6d8d8ee5f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 3.b.c - plot results from full file (RUN THIS CELL AS IS)\n",
    "plt.hist(full_results[2], bins=50)\n",
    "plt.title(\"Distribution of Out-Degree\")\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2335011b-57ae-417b-a682-b3f7f8ce0a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 4: PageRank\n",
    "# Author: Brandon\n",
    "\n",
    "## a. Initialize the Graph\n",
    "One of the challenges of performing distributed graph computation is that you must pass the entire graph structure through each iteration of your algorithm. As usual, we seek to design our computation so that as much work as possible can be done using the contents of a single record. In the case of PageRank, we'll need each record to include a node, its list of neighbors and its (current) rank. In this question you'll initialize the graph by creating a record for each dangling node and by setting the initial rank to 1/N for all nodes. \n",
    "\n",
    "__`NOTE:`__ Your solution should _not_ hard code \\\\(N\\\\).\n",
    "\n",
    "### Q4.a Tasks:\n",
    "* __a) Multiple Choice:__ What is \\\\(N\\\\)? \n",
    "\n",
    "* __b) Multiple Choices:__ Using the analogy of the infinite random web-surfer, how do we use \\\\(\\frac{1}{N}\\\\)? [__HINT:__ _select 2 answers_]\n",
    "\n",
    "* __c) Code:__ Fill in the missing code below to create a Spark job that:\n",
    "  * parses each input record\n",
    "  * creates a new record for any dangling nodes and sets it edges (neighbors) to be a empty [__HINT:__ _you should use the same data type with the edges for non-dangling nodes_]\n",
    "  * initializes a rank of 1/N for each node\n",
    "  * returns a pair RDD with records in the format specified by the docstring\n",
    "\n",
    "* __d) Numerical Answer:__ Run the provided code to confirm that your job in `part c` has a record for each node. Your records should match the format specified in the docstring and the count should match what you computed in question 3a. Then answer the question: how many edges does the node `13415942` have? [__`TIP:`__ _you might want to take a moment to write out what expected outputs you should get for the test graph, this will help you know your code works as expected_]\n",
    " \n",
    "__`NOTE:`__ Please observe scalability best practices in the design of your code & comment your work clearly. You will be graded on both the clarity and the design.\n",
    "\n",
    "\n",
    "## b. Iterate until Convergence\n",
    "Finally we're ready to compute the page rank. In this last question you'll write a Spark job that iterates over the initialized graph updating each nodes score until it reaches a convergence threshold. The diagram below gives a visual overview of the process using a 5 node toy graph. Pay particular attention to what happens to the dangling mass at each iteration.\n",
    "\n",
    "<img src='https://github.com/kyleiwaniec/w261_assets/blob/master/images/HW5/PR-illustrated.png?raw=true' width=50%>\n",
    "\n",
    "__`A Note about Notation:`__ The formula above describes how to compute the updated page rank for a node in the graph. The \\\\(P\\\\) on the left hand side of the equation is the new score, and the \\\\(P\\\\) on the right hand side of the equation represents the accumulated mass that was re-distributed from all of that node's in-links. Finally, \\\\(|G|\\\\) is the number of nodes in the graph (which we've elsewhere refered to as \\\\(N\\\\)).\n",
    "\n",
    "### Q4.b Tasks:\n",
    "* __a) Multiple Choices:__ In terms of the infinite random walk analogy, interpret the meaning of the first term in the PageRank calculation: \\\\(\\alpha * \\frac{1}{|G|}\\\\). \n",
    "    [__Hint__: _select two answers_]\n",
    "\n",
    "* __b) Multiple Choice:__ In the equation for the PageRank calculation above what does \\\\(m\\\\) represent and why do we divide it by \\\\(|G|\\\\)?\n",
    "\n",
    "* __c) Numerical Answer:__ Keeping track of the total probability mass after each update is a good way to confirm that your algorithm is on track. How much should the total mass be after each iteration?\n",
    "\n",
    "* __d) Code:__ Fill in the missing code below to create a Spark job that take the initialized graph as its input then iterates over the graph and for each pass:\n",
    "  * reads in each record and redistributes the node's current score to each of its neighbors\n",
    "  * uses an accumulator to add up the dangling node mass and redistribute it among all the nodes. (_Don't forget to reset this accumulator after each iteration!_)\n",
    "  * uses an accumulator to keep track of the total mass being redistributed.( _This is just for your own check, its not part of the PageRank calculation. Don't forget to reset this accumulator after each iteration._)\n",
    "  * aggregates these partial scores for each node\n",
    "  * applies telportation and damping factors as described in the formula above.\n",
    "  * combine all of the above to compute the PageRank as described by the formula above.\n",
    "\n",
    "* __e) Multiple Choice:__  What is the top ranked node of the `wikiaGraphRDD`?\n",
    "    \n",
    "> TIP: You can check your work for part `d` by looking at the nodes in the top 20-40th positions by PageRank score, which should match the expected output.\n",
    "\n",
    "__WARNING:__ Some pages contain multiple hyperlinks to the same destination, please take this into account when redistributing the mass.\n",
    "\n",
    "__NOTE:__ Please observe scalability best practices in the design of your code & comment your work clearly. You will be graded on both the clarity and the design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab638b99-08ce-4570-92ad-a4fb6cb4d2ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q4.a Student Answers:\n",
    "> __a)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "* The rank for each edge\n",
    "* The total number of edges in the graph\n",
    "* The rank for each node\n",
    "* **The total number of nodes in the graph**\n",
    "* The total number of non-dangling nodes in the graph\n",
    "\n",
    "> __b)__ Highlight the correct answers (Add ** before and after your selected choice):\n",
    "\n",
    "* **To initialize each node's rank**\n",
    "* To identify where each node should converge to\n",
    "* To suggest that the random surfer is equally likely to end her random walk anywhere on the graph\n",
    "* **To suggest that the random surfer is equally likely to start her random walk anywhere on the graph**\n",
    "\n",
    "> __d)__ Numeric Answer: 170\n",
    "\n",
    "### Q4.b Student Answers:\n",
    "> __a)__ Highlight the correct answers (Add ** before and after your selected choice):\n",
    "\n",
    "* alpha is the number of nodes, with an equal probability of being \"jumped to\".\n",
    "* **\\\\(|G|\\\\) is the number of nodes, with an equal probability of being \"jumped to\"**.\n",
    "* **alpha is the teleportation factor (i.e. the probability of a random jump)**\n",
    "* \\\\(|G|\\\\)  is the teleportation factor (i.e. the probability of a random jump)\n",
    "\n",
    "> __b)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "\n",
    "* m is the total mass of all nodes which we distribute to all |G| nodes equally.\n",
    "* **m is the dangling mass which we distribute to all |G| nodes equally.**\n",
    "\n",
    "> __c)__ Numeric Answer: 1\n",
    "\n",
    "> __e)__ Highlight the correct answer (Add ** before and after your selected choice):\n",
    "\n",
    "* 1184351\n",
    "* **13455888**\n",
    "* 4695850\n",
    "* 5051368\n",
    "* 2437837"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "785e5640-4910-4e18-b7fc-09dc7d8dc69e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 4.a.c - job to initialize the graph\n",
    "def initGraph(dataRDD):\n",
    "    \"\"\"\n",
    "    Spark job to read in the raw data and initialize an \n",
    "    adjacency list representation with a record for each\n",
    "    node (including dangling nodes).\n",
    "    \n",
    "    Returns: \n",
    "        graphRDD -  a pair RDD of (node_id , (score, edges))\n",
    "        \n",
    "    NOTE: The score should be a float, but you may want to be \n",
    "    strategic about how format the edges... there are a few \n",
    "    options that can work. Make sure that whatever you choose\n",
    "    is sufficient for Question 8 where you'll run PageRank.\n",
    "    \"\"\"\n",
    "    ############## YOUR CODE HERE ###############\n",
    "\n",
    "    # write any helper functions here\n",
    "\n",
    "    def parse_RDD(dataRDD):\n",
    "            split_result = re.split(r'\\t', dataRDD)\n",
    "            node = split_result[0]\n",
    "            adj_list = []\n",
    "            neighbors = eval(split_result[1])\n",
    "            adj_list = \",\".join([str(key) for key, count in neighbors.items() for _ in range(count)])\n",
    "            \n",
    "            return node, adj_list\n",
    "\n",
    "        # # write your main Spark code here\n",
    "    parsed_rdd = dataRDD.map(parse_RDD).cache()\n",
    "\n",
    "    all_nodes = parsed_rdd.flatMap(lambda x: [x[0]] + x[1].split(',')).distinct()\n",
    "    distinct_node_count = all_nodes.count()\n",
    "    score = 1/distinct_node_count\n",
    "\n",
    "    graphRDD = parsed_rdd.map(lambda x: (int(x[0]), (score, x[1])))\n",
    "\n",
    "    nodes_key = parsed_rdd.flatMap(lambda x: [x[0]]).distinct()\n",
    "    nodes_edges = parsed_rdd.flatMap(lambda x: x[1].split(',')).distinct()\n",
    "    dangling_nodes = nodes_edges.subtract(nodes_key)\n",
    "    dangling_rdd = dangling_nodes.map(lambda node: (int(node), (score, \"\")))\n",
    "\n",
    "    graphRDD = graphRDD.union(dangling_rdd)\n",
    "\n",
    "    ############## (END) YOUR CODE ##############\n",
    "    \n",
    "    return graphRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1a368a-99e1-453d-a698-cbabf15b2293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 4.a.d - run your Spark job on the test graph (RUN THIS CELL AS IS)\n",
    "# Approx time: 1.55 seconds\n",
    "testGraph = initGraph(testRDD).collect()\n",
    "testGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b373f0-605a-4aa9-a938-ae43b9b22ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 4.a.d - run your code on the main graph (RUN THIS CELL AS IS)\n",
    "# Approx time: 8.16 minutes\n",
    "start = time.time()\n",
    "wikiGraphRDD = initGraph(wikiRDD)\n",
    "print(f'... full graph initialized in {time.time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e1392b-448f-478f-9dd3-aa37e905b334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 4.a.d - confirm record format and count (RUN THIS CELL AS IS)\n",
    "# Approx time: 2.40 minutes # 5781290\n",
    "start = time.time()\n",
    "print(f'Total number of records: {wikiGraphRDD.count()}')\n",
    "node = wikiGraphRDD.sortByKey().lookup(13415942) \n",
    "print(\"Node 13415942's initialized record:\", node)\n",
    "print(f\"\\n\\nNode 13415942 has {len(node[0][1].split(','))} outlinks.\")\n",
    "print(f'Time to complete: {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47648cb-aa6e-49ef-aa43-9cdd00b34809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 4.b.d - provided FloatAccumulator class (RUN THIS CELL AS IS)\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "class FloatAccumulatorParam(AccumulatorParam):\n",
    "    \"\"\"\n",
    "    Custom accumulator for use in page rank to keep track of various masses.\n",
    "    \n",
    "    IMPORTANT: accumulators should only be called inside actions to avoid duplication.\n",
    "    We stringly recommend you use the 'foreach' action in your implementation below.\n",
    "    \"\"\"\n",
    "    def zero(self, value):\n",
    "        return value\n",
    "    def addInPlace(self, val1, val2):\n",
    "        return val1 + val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8147ab-7f08-4dfa-9964-cb7f1d11c096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 4.b.d - job to run PageRank\n",
    "def runPageRank(graphInitRDD, alpha = 0.15, maxIter = 10, verbose = True):\n",
    "    \"\"\"\n",
    "    Spark job to implement page rank\n",
    "    Args: \n",
    "        graphInitRDD  - pair RDD of (node_id , (score, edges))\n",
    "        alpha         - (float) teleportation factor\n",
    "        maxIter       - (int) stopping criteria (number of iterations)\n",
    "        verbose       - (bool) option to print logging info after each iteration\n",
    "    Returns:\n",
    "        steadyStateRDD - pair RDD of (node_id, pageRank)\n",
    "    \"\"\"\n",
    "    # teleportation:\n",
    "    a = sc.broadcast(alpha)\n",
    "    \n",
    "    # damping factor:\n",
    "    d = sc.broadcast(1-a.value)\n",
    "    \n",
    "    # initialize accumulators for dangling mass & total mass\n",
    "    mmAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n",
    "    totAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n",
    "    \n",
    "    ############## YOUR CODE HERE ###############\n",
    "    \n",
    "    # write your helper functions here, \n",
    "    # please document the purpose of each clearly \n",
    "    # for reference, the master solution has 5 helper functions.\n",
    "    \n",
    "    def sendRankToNeighbors(node):\n",
    "        #split input from graphInitRDD to node_id, score, and edges \n",
    "        node_id, (rank, neighbors) = node\n",
    "        #Turn the neighbors into a list so it can be counted\n",
    "        neighbors_list = neighbors.split(',')\n",
    "        num_neighbors = len(neighbors_list)\n",
    "        #From the graphInitRDD send the node_id and its associated neighbors to recreate the graph in each iteration\n",
    "        yield (int(node_id), (0.0, neighbors))\n",
    "        \n",
    "        #If a node has neighbours, send out the score associated with each neighbour, also yields each node_id to ensure all node_ids even if it wasn't a neighbour of other node_ids (ex. node 7 - 11 aren't in the neighbors_list for any of the other nodes)\n",
    "        if neighbors_list != ['']:\n",
    "            yield (int(node_id), (0.0,''))\n",
    "            rank_contribution = rank / num_neighbors\n",
    "            for neighbor in neighbors_list:\n",
    "                yield (int(neighbor), (rank_contribution,''))\n",
    "\n",
    "    #accumulate dangling mass\n",
    "    def accumulateDanglingMass(node):\n",
    "        node_id, (rank, neighbors) = node\n",
    "        neighbors = neighbors.split(',')\n",
    "        if neighbors == ['']:\n",
    "            mmAccum.add(rank)\n",
    "    \n",
    "    #accumulate total mass \n",
    "    def accumulateTotalMass(node):\n",
    "        node_id, (rank, neighbors) = node\n",
    "        totAccum.add(rank)\n",
    "\n",
    "    #update the rank for each node using the page rank formula\n",
    "    def updateRank(node, dangling_mass, num_nodes):\n",
    "        node_id, (rank, neighbors) = node\n",
    "        new_rank = (a.value * 1 / num_nodes) + d.value * (dangling_mass / num_nodes + rank)\n",
    "        return (node_id, (new_rank, neighbors))\n",
    "\n",
    "    #format RDD to desired output as shown in example output\n",
    "    def formatoutput(node):\n",
    "        node_id, (rank, neighbors) = node\n",
    "        return (node_id, rank)\n",
    "  \n",
    "    # write your main Spark Job here (including the for loop to iterate)\n",
    "    # for reference, the master solution is 21 lines including comments & whitespace\n",
    " \n",
    "    graphInitRDD = graphInitRDD.cache()\n",
    "    page_rankRDD = graphInitRDD.flatMap(lambda x: [x[0]]) #extract each individual nodes from initilaized graph\n",
    "    page_rankRDD_count = sc.broadcast(page_rankRDD.count()) #count the number of nodes \n",
    "    score = 1 / page_rankRDD_count.value #define the initial score to intialize to all nodes \n",
    "\n",
    "    for i in range(0, maxIter):\n",
    "        mmAccum.value = 0.0\n",
    "        totAccum.value = 0.0\n",
    "        graphInitRDD.foreach(accumulateDanglingMass) #calculate the dangling mass\n",
    "        graphInitRDD.foreach(accumulateTotalMass) # calculate the total mass \n",
    "        dangling_mass = mmAccum.value\n",
    "\n",
    "        updateRDD = graphInitRDD.flatMap(sendRankToNeighbors) \\\n",
    "                                .reduceByKey(lambda x,y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "                                .map(lambda x: updateRank(x, dangling_mass, page_rankRDD_count.value))\n",
    "\n",
    "        graphInitRDD = updateRDD.cache() #cache outputs from each iteration \n",
    "\n",
    "        if verbose: #Keep track of missing mass and total mass\n",
    "            print(f'STEP {i}: missing mass = {mmAccum.value}, total = {totAccum.value}')\n",
    " \n",
    "    steadyStateRDD = graphInitRDD.map(formatoutput)  \n",
    "    ############## (END) YOUR CODE ###############\n",
    "    \n",
    "    return steadyStateRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cba98ac-add1-4f8a-9914-406c45add456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 4.b.d - run PageRank on the test graph (RUN THIS CELL AS IS)\n",
    "# Note: while developing your code you may want turn on the verbose option\n",
    "# Approx time: 11.80 seconds\n",
    "nIter = 20\n",
    "testGraphRDD = initGraph(testRDD)\n",
    "start = time.time()\n",
    "test_results = runPageRank(testGraphRDD, alpha = 0.15, maxIter = nIter, verbose = False)\n",
    "print(f'...trained {nIter} iterations in {time.time() - start} seconds.')\n",
    "print(f'Top 20 ranked nodes:')\n",
    "test_results.takeOrdered(20, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d67f45f2-6aa2-4d11-82c6-ac75aedce484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "__`expected results for the test graph:`__\n",
    "```\n",
    "[(2, 0.3620640495978871),\n",
    " (3, 0.333992700474142),\n",
    " (5, 0.08506399429624555),\n",
    " (4, 0.06030963508473455),\n",
    " (1, 0.04255740809817991),\n",
    " (6, 0.03138662354831139),\n",
    " (8, 0.01692511778009981),\n",
    " (10, 0.01692511778009981),\n",
    " (7, 0.01692511778009981),\n",
    " (9, 0.01692511778009981),\n",
    " (11, 0.01692511778009981)]\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b18601a6-727e-4229-af9d-40019e092fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 4.b.d - run PageRank on the full graph (RUN THIS CELL AS IS)\n",
    "# NOTES: \n",
    "# - wikiGraphRDD should have been computed & cached above!\n",
    "# - Rounding errors after 10 or so decimal places are acceptable\n",
    "# - This will take a long time to run...about 1.4 hours with 2 workers.\n",
    "nIter = 10\n",
    "start = time.time()\n",
    "full_results = runPageRank(wikiGraphRDD, alpha = 0.15, maxIter = nIter, verbose = True)\n",
    "print(f'\\n...trained {nIter} iterations in {time.time() - start} seconds.\\n')\n",
    "\n",
    "print(f'Top 20-40th ranked nodes:\\n')\n",
    "top_40 = full_results.takeOrdered(40, key=lambda x: - x[1])\n",
    "# print results from 20th to 40th highest PageRank\n",
    "for result in top_40[20:]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49276d1d-da1b-4b15-838c-3a168f90f1c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "**Expected output for pages in top 20-40:**\n",
    "```\n",
    "(9386580, 0.0002956970419566159)\n",
    "(15164193, 0.0002843018514565407)\n",
    "(7576704, 0.00028070286946292773)\n",
    "(12074312, 0.0002789390949918953)\n",
    "(3191491, 0.0002733179890615315)\n",
    "(2797855, 0.0002728954946638793)\n",
    "(11253108, 0.000272424060446348)\n",
    "(13725487, 0.00027020945537652716)\n",
    "(2155467, 0.0002664929816794768)\n",
    "(7835160, 0.00026125677808592145)\n",
    "(4198751, 0.00025892998307531953)\n",
    "(9276255, 0.0002546338645363715)\n",
    "(10469541, 0.0002537014715361682)\n",
    "(11147327, 0.00025154645090952246)\n",
    "(5154210, 0.00024927029620654557)\n",
    "(2396749, 0.0002481158912959703)\n",
    "(3603527, 0.0002465136725657213)\n",
    "(3069099, 0.00023478481004084842)\n",
    "(12430985, 0.0002334928892642041)\n",
    "(9355455, 0.00022658004696908508)\n",
    "```\n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16e9d1e2-1371-4c58-b8a4-5e96e700d550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 4.b.e - What is the top ranked node of the wikiaGraphRDD - (RUN THIS CELL AS IS)\n",
    "top_40_RDD = sc.parallelize(top_40)\n",
    "top_40_RDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c12a3c17-a607-4df0-bb19-5e567fea710d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 5 - GraphFrames\n",
    "# Author: David\n",
    "\n",
    "GraphFrames is a graph library which is built on top of the Spark DataFrames API. If you're doing graph-based analysis in the future, this is a great tool that you can use. GraphFrames does calculations like PageRank is about 25% of the time that you can achieve using the map-reduce paradigm you've used up to this point. The goal of this final question is to expose you to this tool so that you at least have some knowledge of its basic functionality.\n",
    "\n",
    "### Q5 Tasks\n",
    "* __a) Code:__ Extract the vertices and edges of the `wikiRDD` and create two dataframes `v` and `e`. For `e` we are providing the helper function.\n",
    "* __b) Numerical Answer:__ What is the vertice number 5?\n",
    "* __c) Code:__ Compute the out-degree Dataframe using `e`. [__HINT__: Use the GroupBy method]\n",
    "* __d) Numerical Answer:__ What's the out-degree of node 2552\n",
    "* __e) Code:__ Complete the code to run the GraphFrames implementation of PageRank and get the top 20 nodes. \n",
    "* __f) Multiple Choices:__ There are some similarities but also differences between this implementation results and the RDD one. Select the options that are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c468b099-b439-49a3-848e-a6136c492c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Q5 Student Answers:\n",
    "> __b)__ Numeric Answer: 201029\n",
    "\n",
    "> __d)__ Numeric Answer: 21\n",
    "\n",
    "> __f)__ Highlight the correct answers (Add ** before and after your selected choice):\n",
    "\n",
    "* The top 5 ranked nodes are the same in both implementations.\n",
    "* **The top 2 ranked nodes are the same in both implementations.**\n",
    "* **Graphframe implementation doesn't normalize the ranks, while RDD does.**\n",
    "* RDD implementation doesn't normalize the ranks, while Graphframe does.  \n",
    "* **RDD implementation it's slower but it considers the weights of the nodes while Graphframes it's faster but assumes weights 1.**\n",
    "* RDD implementation and Graphframe implementation are exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d2a2af3-0da2-44c7-bc9b-7514c2e30633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 5.a -  load the data into Spark RDDs for convenience of use later (RUN THIS CELL AS IS)\n",
    "DF = wikiRDD.map(lambda x: (x.split('\\t')[0], ast.literal_eval(x.split('\\t')[1]))).toDF()\n",
    "display(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1246ac-dbfe-419a-8a70-be63784964b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 5.a - Create the vertices dataframes\n",
    "# HINT: Start from the DF given in the previous cell, the column _1 will be your id. For vertices, you only need a single value per node.\n",
    "############## YOUR CODE HERE ###############\n",
    "v = DF.selectExpr(\"_1 as id\").distinct()\n",
    "############## END YOUR CODE ###############\n",
    "display(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdcf35dd-6685-4c46-b952-0741a1445483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 5.a - Create the edges dataframe\n",
    "# Use the following helper function\n",
    "def getEdges(row):\n",
    "    \"\"\"\n",
    "    Get the row from the rdd and extract for each dictionary the id and the destination\n",
    "    \"\"\"\n",
    "    node_id, nodes = row\n",
    "    for node in nodes: \n",
    "        yield int(node_id), int(node)\n",
    "\n",
    "# HINT: There are multiple ways of doing this, easiest way to treat non tabular data is to pass to RDD. Use a flatMap\n",
    "# The columns names of the dataframe should be ['src', 'dst']\n",
    "############## YOUR CODE HERE ###############\n",
    "e = DF.rdd.flatMap(lambda row: getEdges((row['_1'], row['_2'].keys()))).toDF(['src', 'dst'])\n",
    "############## END YOUR CODE ###############\n",
    "\n",
    "display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7cc69f-9255-4e66-b38e-4f220c19978b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 5.c - Create the out-degree dataframe\n",
    "# HINT: Name the column Number of out-degrees\n",
    "############## YOUR CODE HERE ###############\n",
    "def getOutDegrees(row):\n",
    "    \"\"\"\n",
    "    Get the source node and weights from the adjacncy dictionaries\n",
    "    \"\"\"\n",
    "    node_id, nodes = row\n",
    "    for _, weight in nodes.items(): \n",
    "        yield int(node_id), int(weight)\n",
    "\n",
    "out_degreeDF = DF.rdd.flatMap(lambda row: getOutDegrees((row['_1'], row['_2']))).groupByKey().mapValues(sum).toDF([\"src\", \"number_of_out_degrees\"])\n",
    "############## END YOUR CODE ###############\n",
    "\n",
    "display(out_degreeDF.filter(F.col('src') == 2552))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0acb8d-549e-498e-8ee8-dde9f1bf9567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# part 5.e - Create a GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# call the pageRank method, with resetProbability=0.15 and maxIter=10\n",
    "############## YOUR CODE HERE ###############\n",
    "results = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "############## END YOUR CODE ###############\n",
    "\n",
    "top_20 = results.vertices.orderBy(F.desc(\"pagerank\")).limit(20)\n",
    "display(top_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "283e566b-62ce-4cbf-a596-61af41a10cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Congratulations, you have completed the last homework for 261! "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Team 4-1 hw5_workbook_Fall2024",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
